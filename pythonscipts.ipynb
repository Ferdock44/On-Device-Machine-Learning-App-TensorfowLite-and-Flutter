{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os #for file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './dataset/train' #setting the base_dir variable to the location of the dataset containing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2872 images belonging to 36 classes.\n",
      "Found 709 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "#now we will do some preprocessing, i.e we are preparing the raw data to make it suitable for a building and training models\n",
    "IMAGE_SIZE = 224 #image size that we are going to set the images in the dataset to\n",
    "BATCH_SIZE = 64 #how many images we are inputing into the neural network at once\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator( #preprocessing out image\n",
    "    rescale = 1./255, #firstly, rescaling it to 1/255 which will make the file size smaller, hence reducing the training time\n",
    "    validation_split=0.2 #secondly, normally a dataset has a test set and a training set, \n",
    "    #validation set is normally to test our neural network,which would give us a measure of accuracy on how well the neural network will do on the predictions.\n",
    "    #here we are telling keras to use 20% for validation and 80% training\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory( #training generator\n",
    "    base_dir, #the directory having the fruits and vegetable photos\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),  #converting images to 224 by 224\n",
    "    batch_size = BATCH_SIZE, #images getting inputed into the neural network through each epoch or each step\n",
    "    subset='training' #the name we will call it\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(  #validation generator\n",
    "    base_dir, \n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")\n",
    "#So as we can see from below, our training generator dataset 2872 images and the validation generator dataset has 709 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 0, 'banana': 1, 'beetroot': 2, 'bell pepper': 3, 'cabbage': 4, 'capsicum': 5, 'carrot': 6, 'cauliflower': 7, 'chilli pepper': 8, 'corn': 9, 'cucumber': 10, 'eggplant': 11, 'garlic': 12, 'ginger': 13, 'grapes': 14, 'jalepeno': 15, 'kiwi': 16, 'lemon': 17, 'lettuce': 18, 'mango': 19, 'onion': 20, 'orange': 21, 'paprika': 22, 'pear': 23, 'peas': 24, 'pineapple': 25, 'pomegranate': 26, 'potato': 27, 'raddish': 28, 'soy beans': 29, 'spinach': 30, 'sweetcorn': 31, 'sweetpotato': 32, 'tomato': 33, 'turnip': 34, 'watermelon': 35}\n"
     ]
    }
   ],
   "source": [
    "#Next we have to create a labels.txt file that will hold all our labels (important for Flutter)\n",
    "print(train_generator.class_indices) #prints every single key and class of that dataset\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys())) #print all these keys as a list of labels into a text file called labels.txt\n",
    "with open('labels.txt', 'w') as f: #writes to the labels.txt file, and if it doesnt exists, it creates one, and if it does exist, it will overrite it. (thats what 'w' is for)\n",
    "    f.write(labels)\n",
    "\n",
    "#preprocessing of neural networks is hence complete and now its time to build our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a neural network using transfer learning method where we take a pretrained neural network called MobileNetV2 which is a convolutional neural network architecture that seeks to perform well on mobile devices and can predict up to 80 different classes\n",
    "#we are going to have a base model on top of which we are going to add pre trained neural network to have it predict the classes we want\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3) \n",
    "base_model = tf.keras.applications.MobileNetV2( #grabbing pretrained neural network of choice\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False, #this will freeze all the weights, because we dont have to retrain and change the weights, instead just add on to the MobileNetV2 CNN, so it clasiffies 5 classes instead of 80\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=False #this freezes all the neurons for our base model\n",
    "model = tf.keras.Sequential([ #neural networks act in a sequence of layers, so we add layers as we want\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(32,3, activation = 'relu'), #This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. Bascially its understanding the patterns inside the image\n",
    "  tf.keras.layers.Dropout(0.2), #This layer prevents Neural Networks from Overfitting, i.e being too precise to a point where the NN is only able to recognize images that are present in the dataset\n",
    "  tf.keras.layers.GlobalAveragePooling2D(), #This layer calculates the average output of each feature map in the previous layer, thus reducing the data significantly and preparing the model for the final layer\n",
    "  tf.keras.layers.Dense(36, #no.of classes\n",
    "                        activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), #Adam is a popular optimiser, designed specifically for training deep neural networks\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 4/45 [=>............................] - ETA: 1:43 - loss: 4.0424 - accuracy: 0.0505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashdeshmukh/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/45 [==========>...................] - ETA: 1:11 - loss: 3.6127 - accuracy: 0.0989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashdeshmukh/opt/anaconda3/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:792: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 160s 4s/step - loss: 3.2315 - accuracy: 0.1669 - val_loss: 1.8783 - val_accuracy: 0.4922\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 161s 4s/step - loss: 1.7911 - accuracy: 0.5172 - val_loss: 1.3584 - val_accuracy: 0.6417\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 170s 4s/step - loss: 1.1478 - accuracy: 0.6951 - val_loss: 0.9765 - val_accuracy: 0.7179\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 171s 4s/step - loss: 0.7139 - accuracy: 0.7973 - val_loss: 0.8758 - val_accuracy: 0.7405\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 174s 4s/step - loss: 0.4740 - accuracy: 0.8656 - val_loss: 0.7927 - val_accuracy: 0.7616\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 176s 4s/step - loss: 0.3186 - accuracy: 0.9057 - val_loss: 0.7516 - val_accuracy: 0.7701\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 180s 4s/step - loss: 0.2227 - accuracy: 0.9450 - val_loss: 0.7509 - val_accuracy: 0.7786\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 192s 4s/step - loss: 0.1566 - accuracy: 0.9640 - val_loss: 0.7548 - val_accuracy: 0.7814\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 184s 4s/step - loss: 0.1231 - accuracy: 0.9699 - val_loss: 0.7718 - val_accuracy: 0.7757\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 179s 4s/step - loss: 0.1071 - accuracy: 0.9776 - val_loss: 0.8128 - val_accuracy: 0.7856\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 #higher the epochs, more accurate is the NN, however it could cause Overfitting, if too high\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs = epochs, \n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    }
   ],
   "source": [
    "#now that we have our neural network trained with tensorflow and keras, we can export it \n",
    "saved_model_dir = '' #means current directory\n",
    "tf.saved_model.save(model, saved_model_dir) #saves to the current directory\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
    "tflite_model = converter.convert() #converts our model into a .tflite model which flutter uses for ondevice machine learning\n",
    "\n",
    "with open('model.tflite', 'wb') as f: #to write the converted model into a file, written as binary so add 'wb' instead of 'w'\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use below codes to download files locally if using google colab\n",
    "#from google.colab import files\n",
    "#files.download('model.tflite')\n",
    "#files.download('labels.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
